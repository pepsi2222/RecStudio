# for training all models
learning_rate: 0.001
weight_decay: 0
learner: adam
scheduler: ~
grad_clip_norm: ~
epochs: 1000
batch_size: 1024
num_workers: 0 # please do not use this parameter, slowing down the training process
gpu: 1
num_threads: 10
accelerator: gpu
seed: 2022

# used for training tower-based model
#ann: {index: 'IVFx,Flat', parameter: ~}  ## 1 HNSWx,Flat; 2 Flat; 3 IVFx,Flat ## {nprobe: 1}  {efSearch: 1}
ann: ~

sampling_method: none #[none, dns, brute, sir, toprand, top&rand, dns&rand]
# sampler: ~  # [uniform, popularity, midx_uni, midx_pop, cluster_uni, cluster_pop, retriever_ipts, retriever_dns]
# negative_count: 1
# sampling_method: ~
# sampling_temperature: 1.0
# excluding_hist: False
init_method: xavier_normal
init_range: ~

# the sampler is configured for dataset
dataset_sampler: ~
dataset_neg_count: ~

negative_count: ~ # negative sample number in training procedure
excluding_hist: False

embed_dim: 64
item_bias: False

# `split_num` is set when the number of items are enormous, which will cause out-of-memory
# problem. The `split_num` represents that the item vectors should be cut into several parts
# to calculate scores step by step.
split_num: ~

# used for evaluating tower-based model
eval_batch_size: 128
val_n_epoch: 1
split_ratio: [0.8,0.1,0.1]
test_metrics: [recall, precision, map, ndcg, mrr, hit]
val_metrics: [ndcg, recall]
topk: 100
cutoff: [10, 20, 5]
early_stop_mode: max
early_stop_patience: 10

save_path: './saved/'
tensorboard_path: './tensorboard'
